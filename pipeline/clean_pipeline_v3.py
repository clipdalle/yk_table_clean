"""
æ’éº¦äººå‘˜ & ä¸»æŒäººå‘˜å­—æ®µæ¸…æ´— Pipeline
ä½¿ç”¨ Gemini API æ‰¹é‡è§£ææ··ä¹±çš„æ–‡æœ¬ï¼Œç”Ÿæˆäººå·¥æ ¡éªŒè¡¨
"""

import pandas as pd
import json
import time
import sys
import os
from pathlib import Path
from typing import Dict, List
from tenacity import retry, stop_after_attempt, wait_fixed

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pipeline.llm.llm_client import _get_client
from pipeline.prompts_v3 import PROMPT_BATCH
from global_config import TEMPERATURE, BATCH_SIZE, CURRENT_MODEL, COLS_CONFIG



def get_temp_path(filename):
    import tempfile
    temp_dir = tempfile.gettempdir()
    return os.path.join(temp_dir, filename)
    
# è¯»å–å·²çŸ¥äººåæ¸…å•
with open('known_names_select.txt', 'r', encoding='utf-8') as f:
    known_names_select = f.read().splitlines()
    known_names_select = [name.strip() for name in known_names_select if name.strip()]
known_names_select = list(set(known_names_select))

print(f"âœ… å·²åŠ è½½ {len(known_names_select)} ä¸ªå·²çŸ¥äººå")
print(f"äººåæ¸…å•: {known_names_select[:10]}..." if len(known_names_select) > 10 else f"äººåæ¸…å•: {known_names_select}")


# ========== æ ¸å¿ƒå‡½æ•° ==========
import traceback 

@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def parse_batch(batch_df: pd.DataFrame, assist_known_names: List[str]) -> List[Dict]:
    """
    æ‰¹é‡è§£æå¤šè¡Œæ•°æ®ï¼ˆä¸€æ¬¡ API è°ƒç”¨ï¼‰
    
    Args:
        batch_df: æ‰¹æ¬¡æ•°æ®æ¡†
    
    Returns:
        è§£æç»“æœåˆ—è¡¨
    """
    # æ„å»ºæ‰¹é‡è¾“å…¥æ•°æ®
    batch_data = []
    for idx, row in batch_df.iterrows():
        batch_data.append({
            "è¡Œå·": idx,
            "å…åä¸­æ–‡": row['å…åä¸­æ–‡'],
            "æ—¥æœŸ": row['æ—¥æœŸï¼ˆå¿…å¡«ï¼‰'],
            "ä¸»æŒ": row['ä¸»æŒï¼ˆå¿…å¡«ï¼‰'],
            "æ’éº¦äººå‘˜": row['æ’éº¦äººå‘˜ï¼ˆå¿…å¡«ï¼‰']
        })
    
    # æ„å»º prompt
    prompt = PROMPT_BATCH.format(
        batch_data=json.dumps(batch_data, ensure_ascii=False, indent=2),
        known_names=json.dumps(assist_known_names, ensure_ascii=False, indent=2)
    )
    
    try:
        # è°ƒç”¨ LLM
        response = model_client.get_completion(prompt, temperature=TEMPERATURE)
        
        # æ¸…ç† markdown ä»£ç å—
        response = response.strip()
        if response.startswith('```json'):
            response = response[7:]
        if response.startswith('```'):
            response = response[3:]
        if response.endswith('```'):
            response = response[:-3]
        response = response.strip()
        
        # è§£æ JSON
        results = json.loads(response)
        
        # éªŒè¯ç»“æœ
        if not isinstance(results, list):
            raise ValueError("è¿”å›ç»“æœä¸æ˜¯æ•°ç»„")
        
        return results
    
    except Exception as e:
        # æ‰¹é‡è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        print(f"\nâš ï¸ æ‰¹é‡è§£æå¤±è´¥: {traceback.format_exc()}")
        print(f"\nåŸå§‹è¾“å…¥æ•°æ®:")
        print(json.dumps(batch_data, ensure_ascii=False, indent=2))
        print(f"\nLLM å“åº”:")
        print(response[:500] if 'response' in locals() else "æ— å“åº”")
        
        return [
            {
                "è¡Œå·": idx,
                "ä¸»æŒ": {
                    "ä¸»æŒäººå‘˜åˆ—è¡¨": []
                },
                "æ’éº¦": {
            "æ’éº¦äººå‘˜åˆ—è¡¨": [],
            "ç¼ºå¸­äººæ•°": 0,
                    "ç½®ä¿¡åº¦": "low"
                },
                "é”™è¯¯": f"æ‰¹é‡è§£æå¤±è´¥: {str(e)[:50]}"
            }
            for idx in batch_df.index
        ]


def further_split(names: List[str], split_char: str = '-') -> List[str]:
    res = []
    for name in names:
        res.extend(name.split(split_char))
    return res
 

def batch_parse_fields(
    df: pd.DataFrame, 
    date_str_from_file: str, 
    strict_date_filter: bool = False,
    assist_known_names: List[str] = None) -> pd.DataFrame:
    """
    æ‰¹é‡è§£ææ’éº¦äººå‘˜å’Œä¸»æŒäººå‘˜å­—æ®µï¼ˆæ¯ä¸ª batchï¼šæ„é€ è¾“å…¥ -> è°ƒç”¨ LLM -> è§£æ -> ç›´æ¥å†™å› DataFrameï¼‰
    """
    total = len(df)
    num_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE

    print(f"å¼€å§‹æ‰¹é‡è§£æ {total} æ¡è®°å½•...")
    print(f"æ‰¹æ¬¡è®¾ç½®: æ¯æ‰¹ {BATCH_SIZE} è¡Œï¼Œå…± {num_batches} æ‰¹")
    print(f"é¢„è®¡è€—æ—¶: çº¦ {num_batches * 0.5:.1f} åˆ†é’Ÿ")

    # ç¡®ä¿ç›®æ ‡åˆ—å­˜åœ¨
    target_columns = [
        'ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ',
        'æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ',
        'æ’éº¦å‡ºå¸­äººæ•°_AIè§£æ',
        'æ’éº¦ç¼ºå¸­äººæ•°_AIè§£æ',
        'æ’éº¦ç½®ä¿¡åº¦_AIè§£æ',
        'æ ‡å‡†åŒ–æ—¥æœŸ_AIè§£æ',
        'æ—¥æœŸåŒ¹é…æ ‡å¿—_AIè§£æ',
        'ä¸»æŒé”™è¯¯_AIè§£æ',
        'æ’éº¦é”™è¯¯_AIè§£æ',
    ]
    for col in target_columns:
        if col not in df.columns:
            df[col] = ''

    total_start_ts = time.time()

    for batch_idx in range(num_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min((batch_idx + 1) * BATCH_SIZE, total)

        batch_df = df.iloc[start_idx:end_idx]

        print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} (ç¬¬ {start_idx+1}-{end_idx} è¡Œ)...", end='')
        batch_start_ts = time.time()

        batch_results = parse_batch(batch_df, assist_known_names=assist_known_names)
            
        # 2) é€è¡Œå†™å›
        for result in batch_results:
            idx = result.get('è¡Œå·')
            host_data = result['ä¸»æŒ']
            paimai_data = result['æ’éº¦']
            error_msg = result.get('é”™è¯¯', '')

            host_list = further_split(host_data['ä¸»æŒäººå‘˜åˆ—è¡¨'], split_char='-')
            paimai_list = further_split(paimai_data['æ’éº¦äººå‘˜åˆ—è¡¨'], split_char='-')
            lack_num = paimai_data['ç¼ºå¸­äººæ•°']
            conf = paimai_data['ç½®ä¿¡åº¦']
            row_standard_date_str = result['æ ‡å‡†åŒ–æ—¥æœŸ']

            df.at[idx, 'ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ'] = '|'.join(host_list)
            df.at[idx, 'æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ'] = '|'.join(paimai_list)
            df.at[idx, 'æ’éº¦å‡ºå¸­äººæ•°_AIè§£æ'] = len(paimai_list)
            df.at[idx, 'æ’éº¦ç¼ºå¸­äººæ•°_AIè§£æ'] = lack_num
            df.at[idx, 'æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] = conf
            df.at[idx, 'æ ‡å‡†åŒ–æ—¥æœŸ_AIè§£æ'] = row_standard_date_str
            df.at[idx, 'æ—¥æœŸåŒ¹é…æ ‡å¿—_AIè§£æ'] = (row_standard_date_str == date_str_from_file)
            df.at[idx, 'ä¸»æŒé”™è¯¯_AIè§£æ'] = error_msg
            df.at[idx, 'æ’éº¦é”™è¯¯_AIè§£æ'] = error_msg

            print(row_standard_date_str, date_str_from_file)

        elapsed = time.time() - batch_start_ts
        print(f" å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f}s")



        # é™æµä¿æŠ¤
        if batch_idx < num_batches - 1:
            time.sleep(0.5)

    total_elapsed = time.time() - total_start_ts
    print(f"\nâœ… è§£æå®Œæˆï¼æ€»ç”¨æ—¶ {total_elapsed/60:.2f} åˆ†é’Ÿï¼ˆ{total_elapsed:.1f} ç§’ï¼‰")
    
    # å¯é€‰çš„ä¸¥æ ¼è¿‡æ»¤ï¼šåªä¿ç•™åŒ¹é…æ—¥æœŸçš„è¡Œï¼ˆç”¨äºç»Ÿè®¡/å¯¼å‡ºï¼‰
    if strict_date_filter:
        before_cnt = len(df)
        df = df[df['æ ‡å‡†åŒ–æ—¥æœŸ_AIè§£æ'] == date_str_from_file]

        # print(date_str_from_file)
        # df.to_excel('debug.xlsx', index=False)
        after_cnt = len(df)
        print(f"ğŸ“† ä¸¥æ ¼æ—¥æœŸè¿‡æ»¤: ä»…ä¿ç•™ æ ‡å‡†åŒ–æ—¥æœŸ=={date_str_from_file} çš„è¡Œ {after_cnt}/{before_cnt}")
    
    return df


def save_cleaned_data(df: pd.DataFrame, output_path: str):
    """
    ä¿å­˜æ¸…æ´—åçš„æ•°æ®ï¼ˆæŒ‰ COLS_CONFIG æ’åºåˆ—ï¼‰

    Args:
        df: åŒ…å«è§£æç»“æœçš„æ•°æ®æ¡†
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # æŒ‰ COLS_CONFIG é‡æ–°æ’åˆ—åˆ—
    col_order = [col['col_name'] for col in COLS_CONFIG if col['col_name'] in df.columns]
    # æ·»åŠ ä¸åœ¨ COLS_CONFIG ä¸­çš„åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰
    remaining_cols = [col for col in df.columns if col not in col_order]
    final_col_order = col_order + remaining_cols

    df_sorted = df[final_col_order]

    # ä¿å­˜ä¸ºæ™®é€š Excel
    df_sorted.to_excel(output_path, index=False)

    print(f"âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜: {output_path}")
    print(f"   - åŸå§‹åˆ—: {len(df_sorted.columns) - 7} åˆ—")
    print(f"   - æ–°å¢åˆ—: 7 åˆ—ï¼ˆä¸»æŒx1ã€æ’éº¦x4ã€é”™è¯¯x2ï¼‰")
    print(f"   - æ€»åˆ—æ•°: {len(df_sorted.columns)} åˆ—")
    print(f"   - åˆ—å·²æŒ‰ CONFIG.COLS_CONFIG æ’åº")


def normalize_ascii_lower(s: str) -> str:
    """
    ä»…å°†è‹±æ–‡å­—æ¯ A-Z è½¬ä¸ºå°å†™ï¼Œå…¶ä»–å­—ç¬¦ï¼ˆå«ä¸­æ–‡ï¼‰ä¿æŒä¸å˜
    
    Args:
        s: è¾“å…¥å­—ç¬¦ä¸²
        
    Returns:
        æ ‡å‡†åŒ–åçš„å­—ç¬¦ä¸²
    """
    if not s:
        return ''
    
    # ä»…å°†è‹±æ–‡å­—æ¯ A-Z è½¬ä¸ºå°å†™ï¼Œå…¶ä»–å­—ç¬¦ï¼ˆå«ä¸­æ–‡ï¼‰ä¸å˜
    ascii_lower_table = {ord(c): ord(c.lower()) for c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'}
    return str(s).translate(ascii_lower_table).strip()


def dudup_names(names: List[str]) -> List[str]:
    """
    å»é‡äººåï¼ˆä»…è‹±æ–‡å¤§å°å†™ä¸æ•æ„Ÿï¼›ä¸­æ–‡/å…¶ä»–å­—ç¬¦ä¿æŒä¸å˜ï¼‰ã€‚
    - ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„å±•ç¤ºåé¡ºåº
    - è§„èŒƒé”®ï¼šå°† A-Z è½¬å°å†™ï¼Œå…¶å®ƒå­—ç¬¦ä¸å˜
    """
    from collections import OrderedDict

    if not names:
        return []

    norm_to_display = OrderedDict()
    for display in names:
        if display is None:
            continue
        disp = str(display).strip()
        if not disp:
            continue
        norm = normalize_ascii_lower(disp)
        if norm and norm not in norm_to_display:
            norm_to_display[norm] = disp

    return list(norm_to_display.values())


def apply_color_by_value(excel_path, value_color_mapping, output_path=None):
    """
    æ ¹æ®å•å…ƒæ ¼å€¼è¿›è¡Œç€è‰²ï¼Œæ¯ä¸ªé…ç½®é¡¹æ”¯æŒç‹¬ç«‹çš„åŒ¹é…æ¨¡å¼
    
    Args:
        excel_path: Excelæ–‡ä»¶è·¯å¾„
        value_color_mapping: å€¼-é¢œè‰²æ˜ å°„åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ï¼š
            [
                {'cell_value': 'Siri', 'color_code': 'FF0000', 'mode': 'exact'},      # ç²¾ç¡®åŒ¹é…
                {'cell_value': 'ç‹æ‘†æ‘†', 'color_code': '00FF00', 'mode': 'contains_value'},    # åŒ…å«åŒ¹é…
                {'cell_value': 'error', 'color_code': 'FFFF00', 'mode': 'exact'}      # ç²¾ç¡®åŒ¹é…
            ]
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¦†ç›–åŸæ–‡ä»¶
    """
    import openpyxl
    from openpyxl.styles import PatternFill
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶
    if output_path is None:
        output_path = excel_path
    
    # åŠ è½½å·¥ä½œç°¿
    wb = openpyxl.load_workbook(excel_path)
    ws = wb.active
    
    # éå†æ‰€æœ‰å•å…ƒæ ¼è¿›è¡Œç€è‰²
    colored_count = 0
    for row in ws.iter_rows():
        for cell in row:
            if cell.value is not None:
                cell_value_str = str(cell.value).strip()
                
                # éå†æ¯ä¸ªé…ç½®é¡¹ï¼Œä½¿ç”¨å…¶ç‹¬ç«‹çš„åŒ¹é…æ¨¡å¼
                for config in value_color_mapping:
                    cell_value = config['cell_value']
                    color_code = config['color_code']
                    mode = config.get('mode', 'exact')  # é»˜è®¤ä½¿ç”¨ç²¾ç¡®åŒ¹é…
                    
                    # æ ¹æ®åŒ¹é…æ¨¡å¼è¿›è¡Œåˆ¤æ–­
                    should_color = False
                    if mode == 'exact':
                        # ç²¾ç¡®åŒ¹é…ï¼šstr == str
                        should_color = (cell_value_str == cell_value)
                    elif mode == 'contains_value':
                        # åŒ…å«åŒ¹é…ï¼šcell_value in cell_value_str
                        should_color = (cell_value in cell_value_str)
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„åŒ¹é…æ¨¡å¼: {mode}ï¼Œè¯·ä½¿ç”¨ 'exact' æˆ– 'contains_value'")
                    
                    # å¦‚æœåŒ¹é…æˆåŠŸï¼Œè¿›è¡Œç€è‰²
                    if should_color:
                        cell.fill = PatternFill(
                            start_color=color_code,
                            end_color=color_code,
                            fill_type='solid'
                        )
                        colored_count += 1
                        break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤ç€è‰²
    
    # ä¿å­˜æ–‡ä»¶
    wb.save(output_path)
    wb.close()
    
    print(f"âœ… å·²æ ¹æ®å€¼ç€è‰²å®Œæˆ")
    print(f"   - è¾“å…¥æ–‡ä»¶: {excel_path}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   - ç€è‰²å•å…ƒæ ¼æ•°: {colored_count}")
    print(f"   - é¢œè‰²æ˜ å°„: {len(value_color_mapping)} ä¸ª")
    
    return colored_count



def save_cleaned_data_with_formula(df: pd.DataFrame, output_path: str):
    """
    ä¿å­˜æ¸…æ´—åçš„æ•°æ®ï¼ˆå¸¦å…¬å¼ç‰ˆæœ¬ï¼Œç»Ÿè®¡è¡¨åœ¨å³ä¾§ï¼‰
    
    Args:
        df: åŒ…å«è§£æç»“æœçš„æ•°æ®æ¡†
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    from openpyxl import Workbook, load_workbook
    from openpyxl.utils import get_column_letter
    from openpyxl.styles import PatternFill, Font, Alignment
    import io

    # æŒ‰ COLS_CONFIG é‡æ–°æ’åˆ—åˆ—
    col_order = [col['col_name'] for col in COLS_CONFIG if col['col_name'] in df.columns]
    remaining_cols = [col for col in df.columns if col not in col_order]
    final_col_order = col_order + remaining_cols
    df_sorted = df[final_col_order]

    # ä¿å­˜ç¬¬ä¸€æ­¥ï¼šæ¸…æ´—åçš„æ•°æ®
    path_step1 = output_path.replace('.xlsx', '.step1_clean_table.xlsx')
    df_sorted.to_excel(path_step1, index=False)
    
    # åŠ è½½å·¥ä½œç°¿
    wb = load_workbook(path_step1)
    ws = wb.active

    # æ‰¾åˆ°ç›¸å…³åˆ—ï¼ˆç”¨äºå³ä¾§ç»Ÿè®¡åŒºï¼‰
    headers = [cell.value for cell in ws[1]]
    host_list_col = None
    paimai_list_col = None
    
    for idx, header in enumerate(headers, 1):
        if header == 'ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ':
            host_list_col = idx
        elif header == 'æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ':
            paimai_list_col = idx
    
    # åˆå§‹åŒ–sorted_namesä¸ºç©ºåˆ—è¡¨
    sorted_names = []
    
    if host_list_col and paimai_list_col:
        host_col_letter = get_column_letter(host_list_col)
        paimai_col_letter = get_column_letter(paimai_list_col)
        data_row_count = ws.max_row
        
        # æå–æ‰€æœ‰å”¯ä¸€çš„äººåï¼ˆä»ä¸»æŒå’Œæ’éº¦åˆ—è¡¨ä¸­ï¼‰
        all_names = set()
        
        # ä»ä¸»æŒåˆ—æå–ï¼ˆä½¿ç”¨ç«–çº¿åˆ†éš”ï¼‰
        for row in range(2, data_row_count + 1):
            cell_value = ws[f'{host_col_letter}{row}'].value
            if cell_value and str(cell_value).strip():
                names = [name.strip() for name in str(cell_value).split('|')]
                all_names.update(names)
        
        # ä»æ’éº¦åˆ—æå–ï¼ˆä½¿ç”¨ç«–çº¿åˆ†éš”ï¼‰
        for row in range(2, data_row_count + 1):
            cell_value = ws[f'{paimai_col_letter}{row}'].value
            if cell_value and str(cell_value).strip():
                names = [name.strip() for name in str(cell_value).split('|')]
                all_names.update(names)
        
        # æŒ‰å­—æ¯é¡ºåºæ’åº
        all_names = dudup_names(all_names)
        sorted_names = sorted(all_names)
        
        # ç»Ÿè®¡è¡¨æ”¾åœ¨å³ä¾§ï¼Œç©ºä¸¤åˆ—åå¼€å§‹
        stats_start_col = len(headers) + 3  # ç©ºä¸¤åˆ—
        stats_col_1 = get_column_letter(stats_start_col)
        stats_col_2 = get_column_letter(stats_start_col + 1)
        stats_col_3 = get_column_letter(stats_start_col + 2)
        
        # è®¾ç½®ç»Ÿè®¡è¡¨è¡¨å¤´ï¼ˆå¸¦æ ·å¼ï¼‰
        header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')  # æ·±è“è‰²
        header_font = Font(bold=True, color='FFFFFF')  # ç™½è‰²ç²—ä½“
        
        ws[f'{stats_col_1}1'] = 'äººå‘˜å§“å'
        ws[f'{stats_col_1}1'].fill = header_fill
        ws[f'{stats_col_1}1'].font = header_font
        ws[f'{stats_col_1}1'].alignment = Alignment(horizontal='center')
        
        ws[f'{stats_col_2}1'] = 'ä¸»æŒæ¬¡æ•°'
        ws[f'{stats_col_2}1'].fill = header_fill
        ws[f'{stats_col_2}1'].font = header_font
        ws[f'{stats_col_2}1'].alignment = Alignment(horizontal='center')
        
        ws[f'{stats_col_3}1'] = 'æ’éº¦æ¬¡æ•°'
        ws[f'{stats_col_3}1'].fill = header_fill
        ws[f'{stats_col_3}1'].font = header_font
        ws[f'{stats_col_3}1'].alignment = Alignment(horizontal='center')
        
        # å¡«å……äººåå’Œå…¬å¼
        for idx, name in enumerate(sorted_names, 2):  # ä»ç¬¬2è¡Œå¼€å§‹
            # äººå‘˜å§“å
            ws[f'{stats_col_1}{idx}'] = name
            
            # ä¸»æŒæ¬¡æ•°ï¼ˆCOUNTIF å…¬å¼ï¼‰
            formula_host = f'=COUNTIF({host_col_letter}:{host_col_letter},"*{name}*")'
            ws[f'{stats_col_2}{idx}'] = formula_host
            
            # æ’éº¦æ¬¡æ•°ï¼ˆCOUNTIF å…¬å¼ï¼‰
            formula_paimai = f'=COUNTIF({paimai_col_letter}:{paimai_col_letter},"*{name}*")'
            ws[f'{stats_col_3}{idx}'] = formula_paimai
        
        # è®¾ç½®åˆ—å®½
        ws.column_dimensions[stats_col_1].width = 20
        ws.column_dimensions[stats_col_2].width = 15
        ws.column_dimensions[stats_col_3].width = 15

    # ä¿å­˜
    path_step2 = output_path.replace('.xlsx', '.add_stats_formula.xlsx')
    wb.save(path_step2)
    wb.close()

    # ç›´æ¥å¯¹æœ€ç»ˆæ–‡ä»¶è¿›è¡Œç€è‰²ï¼ˆä¸ç”Ÿæˆé¢å¤–æ–‡ä»¶ï¼‰
    path_step3 = output_path.replace('.xlsx', '.step3_nowarn.output.xlsx')
    from pipeline.coloring import apply_column_colors_from_config
    apply_column_colors_from_config(path_step2, COLS_CONFIG, path_step3)

    cell_config_list = [
        {'cell_value': 'medium', 'color_code': 'FF0000','mode': 'exact'},
        {'cell_value': 'low', 'color_code': 'FF0000','mode': 'exact'},
        {'cell_value': '9', 'color_code': 'FF0000','mode': 'exact'},
    ]
    for stat_name in sorted_names:
        if normalize_ascii_lower(stat_name) not in known_names_select:
            cell_config_list.append({'cell_value': stat_name, 'color_code': 'FF0000','mode': 'contains_value'})

    apply_color_by_value(path_step3, cell_config_list, output_path=output_path)

    print(f"âœ… å·²ä¿å­˜ï¼ˆå¸¦å…¬å¼ä¸”ç€è‰²ï¼‰: {output_path}")
    print(f"   - æ•°æ®åˆ—: {len(headers)} åˆ—")
    print(f"   - ç»Ÿè®¡è¡¨: åœ¨å³ä¾§ï¼ˆç©º2åˆ—åï¼‰")
    print(f"   - ç»Ÿè®¡äººæ•°: {len(sorted_names) if host_list_col and paimai_list_col else 0} äºº")
    print(f"   - å…¬å¼ä¼šè‡ªåŠ¨ç»Ÿè®¡æ¯ä¸ªäººçš„ä¸»æŒæ¬¡æ•°å’Œæ’éº¦æ¬¡æ•°")
    print(f"   - ä¿®æ”¹äººå‘˜åˆ—è¡¨åï¼Œç»Ÿè®¡ä¼šè‡ªåŠ¨æ›´æ–°")
    print(f"   - åŸºäº CONFIG.COLS_CONFIG é…ç½®ç€è‰²")

    #æ¸…ç†step1å’Œstep2ä¸­é—´æ–‡ä»¶
    Path(path_step1).unlink()
    Path(path_step2).unlink()
    Path(path_step3).unlink()



def generate_statistics(df: pd.DataFrame):
    """
    ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    
    Args:
        df: åŒ…å«è§£æç»“æœçš„æ•°æ®æ¡†
    """
    print("\n" + "="*60)
    print("è§£æç»“æœç»Ÿè®¡")
    print("="*60)
    
    total = len(df)
    
    # ä¸»æŒå­—æ®µç»Ÿè®¡
    print("\nã€ä¸»æŒå­—æ®µã€‘")
    host_errors = (df['ä¸»æŒé”™è¯¯_AIè§£æ'] != '').sum()
    print(f"æˆåŠŸè§£æ: {total - host_errors} ({(total-host_errors)/total*100:.1f}%)")
    print(f"è§£æé”™è¯¯: {host_errors}")
    
    # æ’éº¦å­—æ®µç»Ÿè®¡
    print("\nã€æ’éº¦å­—æ®µã€‘")
    paimai_high = (df['æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] == 'high').sum()
    paimai_medium = (df['æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] == 'medium').sum()
    paimai_low = (df['æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] == 'low').sum()
    paimai_errors = (df['æ’éº¦é”™è¯¯_AIè§£æ'] != '').sum()
    
    print(f"é«˜ç½®ä¿¡åº¦: {paimai_high} ({paimai_high/total*100:.1f}%)")
    print(f"ä¸­ç½®ä¿¡åº¦: {paimai_medium} ({paimai_medium/total*100:.1f}%)")
    print(f"ä½ç½®ä¿¡åº¦: {paimai_low} ({paimai_low/total*100:.1f}%)")
    print(f"è§£æé”™è¯¯: {paimai_errors}")
    
    # å»ºè®®
    need_check = paimai_low + paimai_medium
    print(f"\nå»ºè®®ä¼˜å…ˆæ£€æŸ¥: çº¦ {need_check} æ¡è®°å½•ï¼ˆæ’éº¦ä½+ä¸­ç½®ä¿¡åº¦ï¼‰")

from pathlib import Path
# ========== ä¸»æµç¨‹ ==========
import re 
def extract_chinese(text):
    try:
        return re.sub(r'[^\u4e00-\u9fa5]', '', text)
    except:
        raise ValueError(text)



def get_hall_2_names():
    hall_2_names = {}
    for p in Path('name_list').glob('*.txt'):
        with open(p, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.read().splitlines() if line.strip()]
            hall_2_names[p.stem] = lines
    return hall_2_names
def gen_all_known_names(known_names_from_ui: List[str], selected_halls: List[str]):
    hall_2_names = get_hall_2_names()
    known_names_from_local = []
    for hall in selected_halls:
        known_names_from_local.extend(hall_2_names[hall])

    
    all_names = []
    if known_names_from_ui:
        all_names.extend(known_names_from_ui)
    all_names.extend(known_names_from_local)
    all_names = list(set(all_names))
    return all_names
 

def process_one_file(
        excel_path: str, 
        output_path: str, 
        date_str_from_file: str, 
        strict_date_filter: bool = False, 
        selected_halls: list = None,
        known_names_from_ui: List[str] = None):
    """å¤„ç†å•ä¸ª Excel æ–‡ä»¶å¹¶è¾“å‡ºæœ€ç»ˆç€è‰²æ–‡ä»¶

    Args:
        excel_path: è¾“å…¥ Excel è·¯å¾„
        output_path: æœ€ç»ˆç€è‰²æ–‡ä»¶è·¯å¾„ï¼ˆå¿…å¡«ï¼‰ã€‚
        date_str_from_file: æ—¥æœŸå­—ç¬¦ä¸²
        strict_date_filter: æ˜¯å¦ä¸¥æ ¼æŒ‰æ—¥æœŸç­›é€‰
        selected_halls: é€‰æ‹©çš„å…å·åˆ—è¡¨ï¼Œç”¨äºç­›é€‰æ•°æ®
        known_names_from_ui: ä»å‰ç«¯ä¸Šä¼ çš„å·²çŸ¥äººååˆ—è¡¨
    """
    output_dir = Path(output_path).parent
    # ç¡®ä¿åªåœ¨tempç›®å½•ä¸­åˆ›å»ºç›®å½•
    import tempfile
    temp_dir = tempfile.gettempdir()
    if not str(output_dir).startswith(temp_dir):
        output_dir = Path(tempfile.mkdtemp())
        output_path = str(output_dir / Path(output_path).name)
    output_dir.mkdir(exist_ok=True)
    print("="*60)
    print("æ’éº¦äººå‘˜ & ä¸»æŒäººå‘˜å­—æ®µæ¸…æ´— Pipeline")
    print("="*60)
    print(f"å½“å‰ä½¿ç”¨æ¨¡å‹: {CURRENT_MODEL.upper()}")
    print("="*60)
    
    # 1. è¯»å–æ•°æ®
    print(f"\nğŸ“– è¯»å–æ•°æ®: {excel_path}")
    df = pd.read_excel(excel_path, sheet_name=0)
    print(excel_path, '+++++++++++++++')
    df['å…åä¸­æ–‡'] = df['å…å·ï¼ˆå¿…å¡«ï¼‰'].apply(extract_chinese)
    print(f"   - åŸå§‹è®°å½•æ•°: {len(df)}")
        
    # ç­›é€‰ç‰¹å®šå…å·ï¼ˆå¿…é¡»é€‰æ‹©å…å·ï¼‰
    if not selected_halls or len(selected_halls) == 0:
        raise ValueError("âŒ é”™è¯¯ï¼šå¿…é¡»è‡³å°‘é€‰æ‹©ä¸€ä¸ªå…å·è¿›è¡Œå¤„ç†")
    
    # ä» name_list ç›®å½•è¯»å–è¾…åŠ©åå•
    all_known_names = gen_all_known_names(known_names_from_ui, selected_halls)
        
    # å°†å…å·åˆ—è¡¨æ‹¼æ¥æˆæ­£åˆ™è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ï¼š['é†‰æ˜¥è‰²', 'ç™¾åªšç”Ÿ'] -> 'é†‰æ˜¥è‰²|ç™¾åªšç”Ÿ'
    hall_filter = '|'.join(selected_halls)
    print(f"   - ä½¿ç”¨é€‰æ‹©çš„å…å·ç­›é€‰: {hall_filter}")
    df = df[df['å…å·ï¼ˆå¿…å¡«ï¼‰'].str.contains(hall_filter, na=False)]
    print(f"âœ… è¯»å–å®Œæˆï¼Œç­›é€‰åå…± {len(df)} æ¡è®°å½•")

 
    # 2. æ‰¹é‡è§£æ
    print(f"\nğŸ¤– å¼€å§‹ LLM æ‰¹é‡è§£æ...")
    df_parsed = batch_parse_fields(
        df,
        date_str_from_file=date_str_from_file,
        strict_date_filter=strict_date_filter,
        assist_known_names=all_known_names
    )
    
    # 3. ç”Ÿæˆç»Ÿè®¡
    generate_statistics(df_parsed)
    
    # 4. ä»…ä¿å­˜å¸¦å…¬å¼ç‰ˆæœ¬å¹¶ç”Ÿæˆç€è‰²ç‰ˆæœ¬ï¼ˆä¸­é—´äº§ç‰©ï¼‰
    print(f"\nğŸ“Š ä¿å­˜å¸¦å…¬å¼ç‰ˆæœ¬å¹¶ç€è‰²...")
    save_cleaned_data_with_formula(df_parsed, str(output_path))

    # åœ¨æœ€ç»ˆè·¯å¾„ä¸Šå·²å®Œæˆå†™å…¥ä¸ç€è‰²
    
    print("\n" + "="*60)
    print("âœ… Pipeline æ‰§è¡Œå®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“‹ è¾“å‡ºæ–‡ä»¶ï¼š")
    print(f"1. {output_path}")
    print(f"   - å¸¦å…¬å¼ä¸”ç€è‰²ç‰ˆæœ¬")
    print(f"   - åŸºäº CONFIG.COLS_CONFIG é…ç½®ç€è‰²")
    print(f"\nğŸ’¡ è¯´æ˜ï¼šä»…è¾“å‡ºå¸¦é¢œè‰²ç‰ˆæœ¬ï¼Œä¾¿äºç›´æ¥äººå·¥æ ¡éªŒ")


from datetime import datetime 

@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def get_date_str_from_text(text):
    PROMPT_DATE_EXTRACT = f'''
    å½“å‰æ—¶é—´æ˜¯: {datetime.now().strftime('%Yå¹´')}

    ä½ æ˜¯ä¸€ä¸ªæ—¶é—´æ—¥æœŸè§£æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ï¼Œè¿”å›è¿™ä¸ªæ–‡ä»¶åˆé€‚çš„æ—¥æœŸå­—ç¬¦ä¸²ã€‚
    
    è¾“å…¥æ–‡æœ¬ï¼š{text}
    
    **é‡è¦è§„åˆ™ï¼š**
    1. å¦‚æœæ–‡ä»¶åä¸­**æ²¡æœ‰æ˜ç¡®çš„æ—¥æœŸä¿¡æ¯**ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸² ''
    2. å¦‚æœæ—¥æœŸä¿¡æ¯**æœ‰æ­§ä¹‰**ï¼ˆå¦‚09223å¯èƒ½æ˜¯0922æˆ–0923ï¼‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸² ''
    3. å¦‚æœæ—¥æœŸä¿¡æ¯**ä¸å®Œæ•´**ï¼ˆå¦‚åªæœ‰æœˆä»½æ²¡æœ‰æ—¥æœŸï¼‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸² ''
    4. åªæœ‰**æ˜ç¡®ä¸”æ— æ­§ä¹‰**çš„æ—¥æœŸæ‰è¿”å› YYYYMMDD æ ¼å¼
    5. æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼çš„è¯†åˆ«å’Œè½¬æ¢
    
    è¿”å›æ ¼å¼ä¸º YYYYMMDD çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œä¸ç¡®å®šæ—¶è¿”å›ç©ºå­—ç¬¦ä¸² ''ã€‚
    
    ç¤ºä¾‹ï¼š
    è¾“å…¥ï¼šæ˜ å®¢10æœˆ03æ—¥æ‰“å¡æ•°æ®.xlsxï¼Œ ä¸”æˆ‘ä»¬å·²çŸ¥å½“å¹´æ˜¯2025å¹´ï¼Œåˆ™
    è¾“å‡ºï¼š20251003
    
    è¾“å…¥ï¼šä¸»æŒæ‰“å¡9.27.xlsx
    è¾“å‡ºï¼š20250927
    
    è¾“å…¥ï¼šæ‰“å¡æ•°æ®.xlsx
    è¾“å‡ºï¼š
    
    è¾“å…¥ï¼šä¸»æŒæ‰“å¡09223.xlsx
    è¿™ä¸ªæœ‰æ­§ä¹‰
    è¾“å‡ºï¼š
    
    è¾“å…¥ï¼š10æœˆæ‰“å¡.xlsx
    è¾“å‡ºï¼š
    
    è¾“å…¥ï¼šä¸»æŒæ‰“å¡2024-10-03.xlsx
    è¾“å‡ºï¼š20241003
    
  

    **åªè¿”å›æ—¥æœŸå­—ç¬¦ä¸²æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼**
    '''
    try:
        response = model_client.get_completion(
            PROMPT_DATE_EXTRACT.format(text=text)
        )
        
        # æ¸…ç†å“åº”å†…å®¹
        response = response.strip().replace("'", "")
        
        # éªŒè¯è¿”å›æ ¼å¼
        if response == '':
            return ''
        
        # æ£€æŸ¥æ˜¯å¦ä¸º8ä½æ•°å­—æ ¼å¼
        if len(response) == 8 and response.isdigit():
            return response
        
        # å¦‚æœä¸ç¬¦åˆæ ¼å¼ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return ''
        
    except Exception as e:
        print(f"æ—¥æœŸæå–å‡ºé”™: {e}")
        return ''


model_client = _get_client(model_name=CURRENT_MODEL)


def gen_names():
    
    names = []
    for p in Path(r'F:\vscode_workspace\smart_table\daily_data').glob('*.xlsx'):
        if 'output' in p.stem:
            df = pd.read_excel(p, sheet_name=0)
            for idx, row in df.iterrows():
                host_list_text = row['ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ']
                paimai_list_text = row['æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ']
                if host_list_text and paimai_list_text:
                    # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå¤„ç† NaN ç­‰å¼‚å¸¸å€¼
                    host_list_text = str(host_list_text) if host_list_text is not None else ''
                    paimai_list_text = str(paimai_list_text) if paimai_list_text is not None else ''
                    
                    host_list = [name.strip() for name in host_list_text.split('|') if name.strip()]
                    paimai_list = [name.strip() for name in paimai_list_text.split('|') if name.strip()]
                    for host_name in host_list:
                        names.extend(host_name.split('-'))
                    for paimai_name in paimai_list:
                        names.extend(paimai_name.split('-'))
    names = list(set(names))
    # å†™å…¥tempç›®å½•è€Œä¸æ˜¯å½“å‰ç›®å½•
    import tempfile
    temp_dir = tempfile.gettempdir()
    known_names_path = os.path.join(temp_dir, 'known_names_v2.txt')
    with open(known_names_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(names))
        

 

 
import time 

if __name__ == '__main__':
    # gen_names()
    # raise 
 
    paths = list(Path('daily_data').glob('*.xlsx'))
    for p in paths:
        p = Path(p)
 
        output_path = p.with_suffix(f'.{CURRENT_MODEL}.v3.output.xlsx')
        # if 'v3' in p.stem:
        #     continue
        if 'output' in p.stem:
            continue 

        date_str_from_file = get_date_str_from_text(p.stem)
        from global_config import STRICT_DATE_FILTER
        process_one_file(p, output_path=output_path, date_str_from_file=date_str_from_file, strict_date_filter=STRICT_DATE_FILTER)
        time.sleep(1)
"""
æ’éº¦äººå‘˜ & ä¸»æŒäººå‘˜å­—æ®µæ¸…æ´— Pipeline
ä½¿ç”¨ Gemini API æ‰¹é‡è§£ææ··ä¹±çš„æ–‡æœ¬ï¼Œç”Ÿæˆäººå·¥æ ¡éªŒè¡¨
"""

import pandas as pd
import json
import time
import sys
import os
import tempfile
import traceback
import re
import io
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_fixed
from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import PatternFill, Font, Alignment

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pipeline.llm.llm_client import _get_client
from pipeline.prompts_v3 import PROMPT_BATCH
from global_config import TEMPERATURE, BATCH_SIZE, CURRENT_MODEL, COLS_CONFIG, STRICT_DATE_FILTER 



def get_temp_path(filename):
    temp_dir = tempfile.gettempdir()
    return os.path.join(temp_dir, filename)
    


model_client = _get_client(model_name=CURRENT_MODEL)




@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def parse_batch(batch_df: pd.DataFrame, assist_known_names: List[str]) -> List[Dict]:
    """
    æ‰¹é‡è§£æå¤šè¡Œæ•°æ®ï¼ˆä¸€æ¬¡ API è°ƒç”¨ï¼‰
    
    Args:
        batch_df: æ‰¹æ¬¡æ•°æ®æ¡†
    
    Returns:
        è§£æç»“æœåˆ—è¡¨
    """
    # æ„å»ºæ‰¹é‡è¾“å…¥æ•°æ®
    batch_data = []
    for idx, row in batch_df.iterrows():
        batch_data.append({
            "è¡Œå·": idx,
            "å…åä¸­æ–‡": row['å…åä¸­æ–‡'],
            "æ—¥æœŸ": row['æ—¥æœŸï¼ˆå¿…å¡«ï¼‰'],
            "ä¸»æŒ": row['ä¸»æŒï¼ˆå¿…å¡«ï¼‰'],
            "æ’éº¦äººå‘˜": row['æ’éº¦äººå‘˜ï¼ˆå¿…å¡«ï¼‰']
        })
    
    # æ„å»º prompt
    prompt = PROMPT_BATCH.format(
        batch_data=json.dumps(batch_data, ensure_ascii=False, indent=2),
        known_names=json.dumps(assist_known_names, ensure_ascii=False, indent=2)
    )
    
    try:
        # è°ƒç”¨ LLM
        response = model_client.get_completion(prompt, temperature=TEMPERATURE)
        
        # æ¸…ç† markdown ä»£ç å—
        response = response.strip()
        if response.startswith('```json'):
            response = response[7:]
        if response.startswith('```'):
            response = response[3:]
        if response.endswith('```'):
            response = response[:-3]
        response = response.strip()
        
        # è§£æ JSON
        results = json.loads(response)
        
        # éªŒè¯ç»“æœ
        if not isinstance(results, list):
            raise ValueError("è¿”å›ç»“æœä¸æ˜¯æ•°ç»„")
        
        return results
    
    except Exception as e:
        # æ‰¹é‡è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        print(f"\nâš ï¸ æ‰¹é‡è§£æå¤±è´¥: {traceback.format_exc()}")
        print(f"\nåŸå§‹è¾“å…¥æ•°æ®:")
        print(json.dumps(batch_data, ensure_ascii=False, indent=2))
        print(f"\nLLM å“åº”:")
        print(response[:500] if 'response' in locals() else "æ— å“åº”")
        
        return [
            {
                "è¡Œå·": idx,
                "ä¸»æŒ": {
                    "ä¸»æŒäººå‘˜åˆ—è¡¨": []
                },
                "æ’éº¦": {
            "æ’éº¦äººå‘˜åˆ—è¡¨": [],
            "ç¼ºå¸­äººæ•°": 0,
                    "ç½®ä¿¡åº¦": "low"
                },
                "é”™è¯¯": f"æ‰¹é‡è§£æå¤±è´¥: {str(e)[:50]}"
            }
            for idx in batch_df.index
        ]


def further_split(names: List[str], split_char: str = '-') -> List[str]:
    res = []
    for name in names:
        res.extend(name.split(split_char))
    return res
 

def batch_parse_fields(
    df: pd.DataFrame, 
    date_str_from_file: str, 
    strict_date_filter: bool = False,
    assist_known_names: List[str] = None) -> pd.DataFrame:
    """
    æ‰¹é‡è§£ææ’éº¦äººå‘˜å’Œä¸»æŒäººå‘˜å­—æ®µï¼ˆæ¯ä¸ª batchï¼šæ„é€ è¾“å…¥ -> è°ƒç”¨ LLM -> è§£æ -> ç›´æ¥å†™å› DataFrameï¼‰
    """
    total = len(df)
    num_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE

    print(f"å¼€å§‹æ‰¹é‡è§£æ {total} æ¡è®°å½•...")
    print(f"æ‰¹æ¬¡è®¾ç½®: æ¯æ‰¹ {BATCH_SIZE} è¡Œï¼Œå…± {num_batches} æ‰¹")
    print(f"é¢„è®¡è€—æ—¶: çº¦ {num_batches * 0.5:.1f} åˆ†é’Ÿ")

    # ç¡®ä¿ç›®æ ‡åˆ—å­˜åœ¨
    target_columns = [
        'ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ',
        'æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ',
        'æ’éº¦å‡ºå¸­äººæ•°_AIè§£æ',
        'æ’éº¦ç¼ºå¸­äººæ•°_AIè§£æ',
        'æ’éº¦ç½®ä¿¡åº¦_AIè§£æ',
        'æ ‡å‡†åŒ–æ—¥æœŸ_AIè§£æ',
        'æ—¥æœŸåŒ¹é…æ ‡å¿—_AIè§£æ',
        'ä¸»æŒé”™è¯¯_AIè§£æ',
        'æ’éº¦é”™è¯¯_AIè§£æ',
    ]
    for col in target_columns:
        if col not in df.columns:
            df[col] = ''

    total_start_ts = time.time()

    for batch_idx in range(num_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min((batch_idx + 1) * BATCH_SIZE, total)

        batch_df = df.iloc[start_idx:end_idx]

        print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} (ç¬¬ {start_idx+1}-{end_idx} è¡Œ)...", end='')
        batch_start_ts = time.time()

        batch_results = parse_batch(batch_df, assist_known_names=assist_known_names)
            
        # 2) é€è¡Œå†™å›
        for result in batch_results:
            idx = result.get('è¡Œå·')
            host_data = result['ä¸»æŒ']
            paimai_data = result['æ’éº¦']
            error_msg = result.get('é”™è¯¯', '')

            host_list = further_split(host_data['ä¸»æŒäººå‘˜åˆ—è¡¨'], split_char='-')
            paimai_list = further_split(paimai_data['æ’éº¦äººå‘˜åˆ—è¡¨'], split_char='-')
            lack_num = paimai_data['ç¼ºå¸­äººæ•°']
            conf = paimai_data['ç½®ä¿¡åº¦']
            row_standard_date_str = result['æ ‡å‡†åŒ–æ—¥æœŸ']

            df.at[idx, 'ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ'] = '|'.join(host_list)
            df.at[idx, 'æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ'] = '|'.join(paimai_list)
            df.at[idx, 'æ’éº¦å‡ºå¸­äººæ•°_AIè§£æ'] = len(paimai_list)
            df.at[idx, 'æ’éº¦ç¼ºå¸­äººæ•°_AIè§£æ'] = lack_num
            df.at[idx, 'æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] = conf
            df.at[idx, 'æ ‡å‡†åŒ–æ—¥æœŸ_AIè§£æ'] = row_standard_date_str
            df.at[idx, 'æ—¥æœŸåŒ¹é…æ ‡å¿—_AIè§£æ'] = (row_standard_date_str == date_str_from_file)
            df.at[idx, 'ä¸»æŒé”™è¯¯_AIè§£æ'] = error_msg
            df.at[idx, 'æ’éº¦é”™è¯¯_AIè§£æ'] = error_msg

            print(row_standard_date_str, date_str_from_file)

        elapsed = time.time() - batch_start_ts
        print(f" å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f}s")



        # é™æµä¿æŠ¤
        if batch_idx < num_batches - 1:
            time.sleep(0.5)

    total_elapsed = time.time() - total_start_ts
    print(f"\nâœ… è§£æå®Œæˆï¼æ€»ç”¨æ—¶ {total_elapsed/60:.2f} åˆ†é’Ÿï¼ˆ{total_elapsed:.1f} ç§’ï¼‰")
    
    # å¯é€‰çš„ä¸¥æ ¼è¿‡æ»¤ï¼šåªä¿ç•™åŒ¹é…æ—¥æœŸçš„è¡Œï¼ˆç”¨äºç»Ÿè®¡/å¯¼å‡ºï¼‰
    if strict_date_filter:
        before_cnt = len(df)
        df = df[df['æ ‡å‡†åŒ–æ—¥æœŸ_AIè§£æ'] == date_str_from_file]

        # print(date_str_from_file)
        # df.to_excel('debug.xlsx', index=False)
        after_cnt = len(df)
        print(f"ğŸ“† ä¸¥æ ¼æ—¥æœŸè¿‡æ»¤: ä»…ä¿ç•™ æ ‡å‡†åŒ–æ—¥æœŸ=={date_str_from_file} çš„è¡Œ {after_cnt}/{before_cnt}")
    
    return df


def save_cleaned_data(df: pd.DataFrame, output_path: str):
    """
    ä¿å­˜æ¸…æ´—åçš„æ•°æ®ï¼ˆæŒ‰ COLS_CONFIG æ’åºåˆ—ï¼‰

    Args:
        df: åŒ…å«è§£æç»“æœçš„æ•°æ®æ¡†
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # æŒ‰ COLS_CONFIG é‡æ–°æ’åˆ—åˆ—
    col_order = [col['col_name'] for col in COLS_CONFIG if col['col_name'] in df.columns]
    # æ·»åŠ ä¸åœ¨ COLS_CONFIG ä¸­çš„åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰
    remaining_cols = [col for col in df.columns if col not in col_order]
    final_col_order = col_order + remaining_cols

    df_sorted = df[final_col_order]

    # ä¿å­˜ä¸ºæ™®é€š Excel
    df_sorted.to_excel(output_path, index=False)

    print(f"âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜: {output_path}")
    print(f"   - åŸå§‹åˆ—: {len(df_sorted.columns) - 7} åˆ—")
    print(f"   - æ–°å¢åˆ—: 7 åˆ—ï¼ˆä¸»æŒx1ã€æ’éº¦x4ã€é”™è¯¯x2ï¼‰")
    print(f"   - æ€»åˆ—æ•°: {len(df_sorted.columns)} åˆ—")
    print(f"   - åˆ—å·²æŒ‰ CONFIG.COLS_CONFIG æ’åº")


def normalize_ascii_lower(s: str) -> str:
    """
    ä»…å°†è‹±æ–‡å­—æ¯ A-Z è½¬ä¸ºå°å†™ï¼Œå…¶ä»–å­—ç¬¦ï¼ˆå«ä¸­æ–‡ï¼‰ä¿æŒä¸å˜
    
    Args:
        s: è¾“å…¥å­—ç¬¦ä¸²
        
    Returns:
        æ ‡å‡†åŒ–åçš„å­—ç¬¦ä¸²
    """
    if not s:
        return ''
    
    # ä»…å°†è‹±æ–‡å­—æ¯ A-Z è½¬ä¸ºå°å†™ï¼Œå…¶ä»–å­—ç¬¦ï¼ˆå«ä¸­æ–‡ï¼‰ä¸å˜
    ascii_lower_table = {ord(c): ord(c.lower()) for c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'}
    return str(s).translate(ascii_lower_table).strip()


def dudup_names(names: List[str]) -> List[str]:
    """
    å»é‡äººåï¼ˆä»…è‹±æ–‡å¤§å°å†™ä¸æ•æ„Ÿï¼›ä¸­æ–‡/å…¶ä»–å­—ç¬¦ä¿æŒä¸å˜ï¼‰ã€‚
    - ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„å±•ç¤ºåé¡ºåº
    - è§„èŒƒé”®ï¼šå°† A-Z è½¬å°å†™ï¼Œå…¶å®ƒå­—ç¬¦ä¸å˜
    """
    from collections import OrderedDict

    if not names:
        return []

    norm_to_display = OrderedDict()
    for display in names:
        if display is None:
            continue
        disp = str(display).strip()
        if not disp:
            continue
        norm = normalize_ascii_lower(disp)
        if norm and norm not in norm_to_display:
            norm_to_display[norm] = disp

    return list(norm_to_display.values())


def apply_color_by_value(excel_path, value_color_mapping, output_path=None):
    """
    æ ¹æ®å•å…ƒæ ¼å€¼è¿›è¡Œç€è‰²ï¼Œæ¯ä¸ªé…ç½®é¡¹æ”¯æŒç‹¬ç«‹çš„åŒ¹é…æ¨¡å¼
    
    Args:
        excel_path: Excelæ–‡ä»¶è·¯å¾„
        value_color_mapping: å€¼-é¢œè‰²æ˜ å°„åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ï¼š
            [
                {'cell_value': 'Siri', 'color_code': 'FF0000', 'mode': 'exact'},      # ç²¾ç¡®åŒ¹é…
                {'cell_value': 'ç‹æ‘†æ‘†', 'color_code': '00FF00', 'mode': 'contains_value'},    # åŒ…å«åŒ¹é…
                {'cell_value': 'error', 'color_code': 'FFFF00', 'mode': 'exact'}      # ç²¾ç¡®åŒ¹é…
            ]
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¦†ç›–åŸæ–‡ä»¶
    """
    import openpyxl
    from openpyxl.styles import PatternFill
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶
    if output_path is None:
        output_path = excel_path
    
    # åŠ è½½å·¥ä½œç°¿
    wb = openpyxl.load_workbook(excel_path)
    ws = wb.active
    
    # éå†æ‰€æœ‰å•å…ƒæ ¼è¿›è¡Œç€è‰²
    colored_count = 0
    for row in ws.iter_rows():
        for cell in row:
            if cell.value is not None:
                cell_value_str = str(cell.value).strip()
                
                # éå†æ¯ä¸ªé…ç½®é¡¹ï¼Œä½¿ç”¨å…¶ç‹¬ç«‹çš„åŒ¹é…æ¨¡å¼
                for config in value_color_mapping:
                    cell_value = config['cell_value']
                    color_code = config['color_code']
                    mode = config.get('mode', 'exact')  # é»˜è®¤ä½¿ç”¨ç²¾ç¡®åŒ¹é…
                    
                    # æ ¹æ®åŒ¹é…æ¨¡å¼è¿›è¡Œåˆ¤æ–­
                    should_color = False
                    if mode == 'exact':
                        # ç²¾ç¡®åŒ¹é…ï¼šstr == str
                        should_color = (cell_value_str == cell_value)
                    elif mode == 'contains_value':
                        # åŒ…å«åŒ¹é…ï¼šcell_value in cell_value_str
                        should_color = (cell_value in cell_value_str)
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„åŒ¹é…æ¨¡å¼: {mode}ï¼Œè¯·ä½¿ç”¨ 'exact' æˆ– 'contains_value'")
                    
                    # å¦‚æœåŒ¹é…æˆåŠŸï¼Œè¿›è¡Œç€è‰²
                    if should_color:
                        cell.fill = PatternFill(
                            start_color=color_code,
                            end_color=color_code,
                            fill_type='solid'
                        )
                        colored_count += 1
                        break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤ç€è‰²
    
    # ä¿å­˜æ–‡ä»¶
    wb.save(output_path)
    wb.close()
    
    print(f"âœ… å·²æ ¹æ®å€¼ç€è‰²å®Œæˆ")
    print(f"   - è¾“å…¥æ–‡ä»¶: {excel_path}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   - ç€è‰²å•å…ƒæ ¼æ•°: {colored_count}")
    print(f"   - é¢œè‰²æ˜ å°„: {len(value_color_mapping)} ä¸ª")
    
    return colored_count



def save_cleaned_data_with_formula(df: pd.DataFrame, output_path: str, all_known_names: List[str]):
    """
    ä¿å­˜æ¸…æ´—åçš„æ•°æ®ï¼ˆå¸¦å…¬å¼ç‰ˆæœ¬ï¼Œç»Ÿè®¡è¡¨åœ¨å³ä¾§ï¼‰
    
    Args:
        df: åŒ…å«è§£æç»“æœçš„æ•°æ®æ¡†
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """

    # æŒ‰ COLS_CONFIG é‡æ–°æ’åˆ—åˆ—
    col_order = [col['col_name'] for col in COLS_CONFIG if col['col_name'] in df.columns]
    remaining_cols = [col for col in df.columns if col not in col_order]
    final_col_order = col_order + remaining_cols
    df_sorted = df[final_col_order]

    # ä¿å­˜ç¬¬ä¸€æ­¥ï¼šæ¸…æ´—åçš„æ•°æ®
    path_step1 = output_path.replace('.xlsx', '.step1_clean_table.xlsx')
    df_sorted.to_excel(path_step1, index=False)
    
    # åŠ è½½å·¥ä½œç°¿
    wb = load_workbook(path_step1)
    ws = wb.active

    # æ‰¾åˆ°ç›¸å…³åˆ—ï¼ˆç”¨äºå³ä¾§ç»Ÿè®¡åŒºï¼‰
    headers = [cell.value for cell in ws[1]]
    host_list_col = None
    paimai_list_col = None
    
    for idx, header in enumerate(headers, 1):
        if header == 'ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ':
            host_list_col = idx
        elif header == 'æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ':
            paimai_list_col = idx
    
    # åˆå§‹åŒ–sorted_namesä¸ºç©ºåˆ—è¡¨
    sorted_names = []
    
    if host_list_col and paimai_list_col:
        host_col_letter = get_column_letter(host_list_col)
        paimai_col_letter = get_column_letter(paimai_list_col)
        data_row_count = ws.max_row
        
        # æå–æ‰€æœ‰å”¯ä¸€çš„äººåï¼ˆä»ä¸»æŒå’Œæ’éº¦åˆ—è¡¨ä¸­ï¼‰
        all_names = set()
        
        # ä»ä¸»æŒåˆ—æå–ï¼ˆä½¿ç”¨ç«–çº¿åˆ†éš”ï¼‰
        for row in range(2, data_row_count + 1):
            cell_value = ws[f'{host_col_letter}{row}'].value
            if cell_value and str(cell_value).strip():
                names = [name.strip() for name in str(cell_value).split('|')]
                all_names.update(names)
        
        # ä»æ’éº¦åˆ—æå–ï¼ˆä½¿ç”¨ç«–çº¿åˆ†éš”ï¼‰
        for row in range(2, data_row_count + 1):
            cell_value = ws[f'{paimai_col_letter}{row}'].value
            if cell_value and str(cell_value).strip():
                names = [name.strip() for name in str(cell_value).split('|')]
                all_names.update(names)
        
        # æŒ‰å­—æ¯é¡ºåºæ’åº
        all_names = dudup_names(all_names)
        sorted_names = sorted(all_names)
 
        
        # ç»Ÿè®¡è¡¨æ”¾åœ¨å³ä¾§ï¼Œç©ºä¸¤åˆ—åå¼€å§‹
        stats_start_col = len(headers) + 3  # ç©ºä¸¤åˆ—
        stats_col_1 = get_column_letter(stats_start_col)
        stats_col_2 = get_column_letter(stats_start_col + 1)
        stats_col_3 = get_column_letter(stats_start_col + 2)
        
        # è®¾ç½®ç»Ÿè®¡è¡¨è¡¨å¤´ï¼ˆå¸¦æ ·å¼ï¼‰
        header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')  # æ·±è“è‰²
        header_font = Font(bold=True, color='FFFFFF')  # ç™½è‰²ç²—ä½“
        
        ws[f'{stats_col_1}1'] = 'äººå‘˜å§“å'
        ws[f'{stats_col_1}1'].fill = header_fill
        ws[f'{stats_col_1}1'].font = header_font
        ws[f'{stats_col_1}1'].alignment = Alignment(horizontal='center')
        
        ws[f'{stats_col_2}1'] = 'ä¸»æŒæ¬¡æ•°'
        ws[f'{stats_col_2}1'].fill = header_fill
        ws[f'{stats_col_2}1'].font = header_font
        ws[f'{stats_col_2}1'].alignment = Alignment(horizontal='center')
        
        ws[f'{stats_col_3}1'] = 'æ’éº¦æ¬¡æ•°'
        ws[f'{stats_col_3}1'].fill = header_fill
        ws[f'{stats_col_3}1'].font = header_font
        ws[f'{stats_col_3}1'].alignment = Alignment(horizontal='center')
        
        # å¡«å……äººåå’Œå…¬å¼
        for idx, name in enumerate(sorted_names, 2):  # ä»ç¬¬2è¡Œå¼€å§‹
            # äººå‘˜å§“å
            ws[f'{stats_col_1}{idx}'] = name
            
            # ä¸»æŒæ¬¡æ•°ï¼ˆCOUNTIF å…¬å¼ï¼‰
            formula_host = f'=COUNTIF({host_col_letter}:{host_col_letter},"*{name}*")'
            ws[f'{stats_col_2}{idx}'] = formula_host
            
            # æ’éº¦æ¬¡æ•°ï¼ˆCOUNTIF å…¬å¼ï¼‰
            formula_paimai = f'=COUNTIF({paimai_col_letter}:{paimai_col_letter},"*{name}*")'
            ws[f'{stats_col_3}{idx}'] = formula_paimai
        
        # è®¾ç½®åˆ—å®½
        ws.column_dimensions[stats_col_1].width = 20
        ws.column_dimensions[stats_col_2].width = 15
        ws.column_dimensions[stats_col_3].width = 15

    # ä¿å­˜
    path_step2 = output_path.replace('.xlsx', '.add_stats_formula.xlsx')
    wb.save(path_step2)
    wb.close()

    # ç›´æ¥å¯¹æœ€ç»ˆæ–‡ä»¶è¿›è¡Œç€è‰²ï¼ˆä¸ç”Ÿæˆé¢å¤–æ–‡ä»¶ï¼‰
    path_step3 = output_path.replace('.xlsx', '.step3_nowarn.output.xlsx')
    from pipeline.coloring import apply_column_colors_from_config
    apply_column_colors_from_config(path_step2, COLS_CONFIG, path_step3)

    cell_config_list = [
        {'cell_value': 'medium', 'color_code': 'FF0000','mode': 'exact'},
        {'cell_value': 'low', 'color_code': 'FF0000','mode': 'exact'},
        {'cell_value': '9', 'color_code': 'FF0000','mode': 'exact'},
    ]
    for stat_name in sorted_names:
        if normalize_ascii_lower(stat_name) not in all_known_names:
            cell_config_list.append({'cell_value': stat_name, 'color_code': 'FF0000','mode': 'contains_value'})

    apply_color_by_value(path_step3, cell_config_list, output_path=output_path)

    print(f"âœ… å·²ä¿å­˜ï¼ˆå¸¦å…¬å¼ä¸”ç€è‰²ï¼‰: {output_path}")
    print(f"   - æ•°æ®åˆ—: {len(headers)} åˆ—")
    print(f"   - ç»Ÿè®¡è¡¨: åœ¨å³ä¾§ï¼ˆç©º2åˆ—åï¼‰")
    print(f"   - ç»Ÿè®¡äººæ•°: {len(sorted_names) if host_list_col and paimai_list_col else 0} äºº")
    print(f"   - å…¬å¼ä¼šè‡ªåŠ¨ç»Ÿè®¡æ¯ä¸ªäººçš„ä¸»æŒæ¬¡æ•°å’Œæ’éº¦æ¬¡æ•°")
    print(f"   - ä¿®æ”¹äººå‘˜åˆ—è¡¨åï¼Œç»Ÿè®¡ä¼šè‡ªåŠ¨æ›´æ–°")
    print(f"   - åŸºäº CONFIG.COLS_CONFIG é…ç½®ç€è‰²")

    #æ¸…ç†step1å’Œstep2ä¸­é—´æ–‡ä»¶
    Path(path_step1).unlink()
    Path(path_step2).unlink()
    Path(path_step3).unlink()



def generate_statistics(df: pd.DataFrame):
    """
    ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    
    Args:
        df: åŒ…å«è§£æç»“æœçš„æ•°æ®æ¡†
    """
    print("\n" + "="*60)
    print("è§£æç»“æœç»Ÿè®¡")
    print("="*60)
    
    total = len(df)
    
    # ä¸»æŒå­—æ®µç»Ÿè®¡
    print("\nã€ä¸»æŒå­—æ®µã€‘")
    host_errors = (df['ä¸»æŒé”™è¯¯_AIè§£æ'] != '').sum()
    print(f"æˆåŠŸè§£æ: {total - host_errors} ({(total-host_errors)/total*100:.1f}%)")
    print(f"è§£æé”™è¯¯: {host_errors}")
    
    # æ’éº¦å­—æ®µç»Ÿè®¡
    print("\nã€æ’éº¦å­—æ®µã€‘")
    paimai_high = (df['æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] == 'high').sum()
    paimai_medium = (df['æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] == 'medium').sum()
    paimai_low = (df['æ’éº¦ç½®ä¿¡åº¦_AIè§£æ'] == 'low').sum()
    paimai_errors = (df['æ’éº¦é”™è¯¯_AIè§£æ'] != '').sum()
    
    print(f"é«˜ç½®ä¿¡åº¦: {paimai_high} ({paimai_high/total*100:.1f}%)")
    print(f"ä¸­ç½®ä¿¡åº¦: {paimai_medium} ({paimai_medium/total*100:.1f}%)")
    print(f"ä½ç½®ä¿¡åº¦: {paimai_low} ({paimai_low/total*100:.1f}%)")
    print(f"è§£æé”™è¯¯: {paimai_errors}")
    
    # å»ºè®®
    need_check = paimai_low + paimai_medium
    print(f"\nå»ºè®®ä¼˜å…ˆæ£€æŸ¥: çº¦ {need_check} æ¡è®°å½•ï¼ˆæ’éº¦ä½+ä¸­ç½®ä¿¡åº¦ï¼‰")

# ========== ä¸»æµç¨‹ ========== 
def extract_chinese(text):
    return re.sub(r'[^\u4e00-\u9fa5]', '', text)



def get_hall_2_names():
    hall_2_names = {}
    for p in Path('name_list').glob('*.txt'):
        with open(p, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.read().splitlines() if line.strip()]
            hall_2_names[p.stem] = lines
    return hall_2_names
def gen_all_known_names(known_names_from_ui: List[str], selected_halls: List[str]):
    hall_2_names = get_hall_2_names()
    known_names_from_local = []
    for hall in selected_halls:
        known_names_from_local.extend(hall_2_names[hall])

    
    all_names = []
    if known_names_from_ui:
        all_names.extend(known_names_from_ui)
    all_names.extend(known_names_from_local)
    all_names = list(set(all_names))
    return all_names
 

def process_one_file(
        excel_path: str, 
        output_path: str, 
        date_str_from_file: str, 
        strict_date_filter: bool = False, 
        selected_halls: list = None,
        known_names_from_ui: List[str] = None):
    """å¤„ç†å•ä¸ª Excel æ–‡ä»¶å¹¶è¾“å‡ºæœ€ç»ˆç€è‰²æ–‡ä»¶

    Args:
        excel_path: è¾“å…¥ Excel è·¯å¾„
        output_path: æœ€ç»ˆç€è‰²æ–‡ä»¶è·¯å¾„ï¼ˆå¿…å¡«ï¼‰ã€‚
        date_str_from_file: æ—¥æœŸå­—ç¬¦ä¸²
        strict_date_filter: æ˜¯å¦ä¸¥æ ¼æŒ‰æ—¥æœŸç­›é€‰
        selected_halls: é€‰æ‹©çš„å…å·åˆ—è¡¨ï¼Œç”¨äºç­›é€‰æ•°æ®
        known_names_from_ui: ä»å‰ç«¯ä¸Šä¼ çš„å·²çŸ¥äººååˆ—è¡¨
    """
    output_dir = Path(output_path).parent
    # ç¡®ä¿åªåœ¨tempç›®å½•ä¸­åˆ›å»ºç›®å½•
    import tempfile
    temp_dir = tempfile.gettempdir()
    if not str(output_dir).startswith(temp_dir):
        output_dir = Path(tempfile.mkdtemp())
        output_path = str(output_dir / Path(output_path).name)
    output_dir.mkdir(exist_ok=True)
    print("="*60)
    print("æ’éº¦äººå‘˜ & ä¸»æŒäººå‘˜å­—æ®µæ¸…æ´— Pipeline")
    print("="*60)
    print(f"å½“å‰ä½¿ç”¨æ¨¡å‹: {CURRENT_MODEL.upper()}")
    print("="*60)
    
    # 1. è¯»å–æ•°æ®
    print(f"\nğŸ“– è¯»å–æ•°æ®: {excel_path}")
    df = pd.read_excel(excel_path, sheet_name=0)
    print(excel_path, '+++++++++++++++')
    df['å…åä¸­æ–‡'] = df['å…å·ï¼ˆå¿…å¡«ï¼‰'].apply(extract_chinese)
    print(f"   - åŸå§‹è®°å½•æ•°: {len(df)}")
        
    # ç­›é€‰ç‰¹å®šå…å·ï¼ˆå¿…é¡»é€‰æ‹©å…å·ï¼‰
    if not selected_halls or len(selected_halls) == 0:
        raise ValueError("âŒ é”™è¯¯ï¼šå¿…é¡»è‡³å°‘é€‰æ‹©ä¸€ä¸ªå…å·è¿›è¡Œå¤„ç†")
    
    # ä» name_list ç›®å½•è¯»å–è¾…åŠ©åå•
    all_known_names = gen_all_known_names(known_names_from_ui, selected_halls)
        
    # å°†å…å·åˆ—è¡¨æ‹¼æ¥æˆæ­£åˆ™è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ï¼š['é†‰æ˜¥è‰²', 'ç™¾åªšç”Ÿ'] -> 'é†‰æ˜¥è‰²|ç™¾åªšç”Ÿ'
    hall_filter = '|'.join(selected_halls)
    print(f"   - ä½¿ç”¨é€‰æ‹©çš„å…å·ç­›é€‰: {hall_filter}")
    df = df[df['å…å·ï¼ˆå¿…å¡«ï¼‰'].str.contains(hall_filter, na=False)]
    print(f"âœ… è¯»å–å®Œæˆï¼Œç­›é€‰åå…± {len(df)} æ¡è®°å½•")

    # 2. æ‰¹é‡è§£æ
    print(f"\nğŸ¤– å¼€å§‹ LLM æ‰¹é‡è§£æ...")
    df_parsed = batch_parse_fields(
        df,
        date_str_from_file=date_str_from_file,
        strict_date_filter=strict_date_filter,
        assist_known_names=all_known_names
    )
    
    # 3. ç”Ÿæˆç»Ÿè®¡
    generate_statistics(df_parsed)
    
    # 4. ä»…ä¿å­˜å¸¦å…¬å¼ç‰ˆæœ¬å¹¶ç”Ÿæˆç€è‰²ç‰ˆæœ¬ï¼ˆä¸­é—´äº§ç‰©ï¼‰
    print(f"\nğŸ“Š ä¿å­˜å¸¦å…¬å¼ç‰ˆæœ¬å¹¶ç€è‰²...")
    save_cleaned_data_with_formula(df_parsed, str(output_path), all_known_names)

    # åœ¨æœ€ç»ˆè·¯å¾„ä¸Šå·²å®Œæˆå†™å…¥ä¸ç€è‰²
    
    print("\n" + "="*60)
    print("âœ… Pipeline æ‰§è¡Œå®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“‹ è¾“å‡ºæ–‡ä»¶ï¼š")
    print(f"1. {output_path}")
    print(f"   - å¸¦å…¬å¼ä¸”ç€è‰²ç‰ˆæœ¬")
    print(f"   - åŸºäº CONFIG.COLS_CONFIG é…ç½®ç€è‰²")
    print(f"\nğŸ’¡ è¯´æ˜ï¼šä»…è¾“å‡ºå¸¦é¢œè‰²ç‰ˆæœ¬ï¼Œä¾¿äºç›´æ¥äººå·¥æ ¡éªŒ")


def load_json_from_llm_completion(response_str):
    response = response_str.strip()
    if response.startswith('```json'):
        response = response[7:]
    if response.startswith('```'):
        response = response[3:]
    if response.endswith('```'):
        response = response[:-3]
    response = response.strip()
    return json.loads(response)


def parse_json_markdown(json_markdown: str) -> str:
    """ä»Markdownæ ¼å¼ä¸­æå–JSONå­—ç¬¦ä¸²"""
    #match = re.search(r"```json\s*(.*?)\s*```", json_markdown, re.DOTALL)
    match = re.search(r"```(json)?(.*)```", json_markdown, re.DOTALL)
    json_string = match.group(2)
    return json.loads(json_string)


def is_legal_date_str(date_str: str) -> bool:
    prompt = f"""
    å½“å‰æ—¥æœŸï¼š{datetime.now().strftime('%Yå¹´%mæœˆ')}

    ä½ æ˜¯ä¸€ä¸ªæ—¥æœŸå­—ç¬¦ä¸²åˆæ³•æ€§åˆ¤æ–­ä¸“å®¶ã€‚  
    è¾“å…¥æ˜¯ä¸€ä¸ªäººç±»æ‰‹å†™çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¯èƒ½åŒ…å«ç®€å•ç¬”è¯¯ä½†ä»å¯ç†è§£ä¸ºåˆæ³•æ—¥æœŸï¼Œæˆ–å› é”™è¯¯è€Œæ— æ³•ç†è§£ä¸ºéæ³•æ—¥æœŸã€‚  

    **åˆæ³•æ—¥æœŸå®šä¹‰**ï¼š  
    1. å¯æ˜ç¡®è§£æä¸ºæœ‰æ•ˆæ—¥æœŸï¼Œå³ä½¿åŒ…å«ç®€å•ç¬”è¯¯ï¼Œä¾‹å¦‚ï¼š  
       - "2025å¹´10æœˆ  28æ—¥" â†’ å¯è§£æä¸º "2025å¹´10æœˆ28æ—¥"  
       - "2025å¹´10.28æ—¥" â†’ å¯è§£æä¸º "2025å¹´10æœˆ28æ—¥"  
       - "2025å¹´1028" â†’ å¯è§£æä¸º "2025å¹´10æœˆ28æ—¥"  
       - "20251028" â†’ å¯è§£æä¸º "2025å¹´10æœˆ28æ—¥"  

    **éæ³•æ—¥æœŸå®šä¹‰**ï¼š  
    1. æ—¥æœŸæœ‰æ­§ä¹‰ï¼Œæ— æ³•æ˜ç¡®è§£æï¼Œä¾‹å¦‚ï¼š  
       - "2025å¹´9223" â†’ æ— æ³•åˆ¤æ–­æ˜¯9æœˆ22æ—¥è¿˜æ˜¯9æœˆ23æ—¥  
    2. æ—¥æœŸæ˜æ˜¾æ— æ•ˆï¼Œä¾‹å¦‚ï¼š  
       - "2025å¹´13æœˆ28æ—¥" â†’ 13æœˆä¸å­˜åœ¨  
    3. æ—¥æœŸè¶…å‡ºåˆç†èŒƒå›´ï¼ˆæ—©äº2000å¹´æˆ–æ™šäºå½“å‰å¹´ä»½+3å¹´ï¼‰ï¼Œä¾‹å¦‚ï¼š  
       - "1999å¹´10æœˆ28æ—¥" â†’ æ—©äº2000å¹´  
       - "2028å¹´10æœˆ28æ—¥" â†’ æ™šäºå½“å‰å¹´ä»½+3å¹´ï¼ˆ{int(datetime.now().strftime('%Y')) + 3}å¹´ï¼‰  

    **ä»»åŠ¡**ï¼š  
    åˆ¤æ–­è¾“å…¥æ—¥æœŸå­—ç¬¦ä¸² `{date_str}` æ˜¯å¦ä¸ºåˆæ³•æ—¥æœŸã€‚  
    è¿”å› JSON æ ¼å¼ç»“æœï¼š  
    ```json
    {{
        "is_legal": true,
        "reason": "æ—¥æœŸå­—ç¬¦ä¸²æ˜¯åˆæ³•çš„æ—¥æœŸå­—ç¬¦ä¸²"
    }}
    ```
    """

    
    response = model_client.get_completion(prompt)
    print(response)
    response = parse_json_markdown(response)
    return response
 
def test_illegal():
    dir_path = r'C:\Users\jizai\Documents\xwechat_files\wxid_loq7ea805m2f21_5d13\msg\file\2025-10\daily_data_1006\daily_data_1006'
    for p in Path(dir_path).glob('*.xlsx'):
        if 'output' in p.stem:
            continue
        df = pd.read_excel(p, sheet_name=0)
        date_list = df['æ—¥æœŸï¼ˆå¿…å¡«ï¼‰'].tolist()
        date_list = list(set(date_list))
        for date_str in date_list:
            res = is_legal_date_str(date_str)
            print(p)
            print(date_str)
            print(res)
            print('----------------------------')
            if not res['is_legal']:
                print(date_str)
                print(res['reason'])
        raise ValueError()


def is_legal_date_batch(date_json: str) -> list:
    prompt = f"""
    å½“å‰æ—¥æœŸï¼š{datetime.now().strftime('%Yå¹´%mæœˆ')}

    ä½ æ˜¯ä¸€ä¸ªæ—¥æœŸå­—ç¬¦ä¸²åˆæ³•æ€§åˆ¤æ–­ä¸“å®¶ã€‚  
    è¾“å…¥æ˜¯ä¸€ä¸ª JSON æ ¼å¼çš„ Excel æ•°æ®ï¼ŒåŒ…å«äººç±»æ‰‹å†™çš„æ—¥æœŸå­—ç¬¦ä¸²å’Œå¯¹åº”çš„è¡Œå·ã€‚æ—¥æœŸå¯èƒ½åŒ…å«ç®€å•ç¬”è¯¯ä½†ä»å¯è§£æä¸ºåˆæ³•æ—¥æœŸï¼Œæˆ–å› é”™è¯¯è¢«åˆ¤å®šä¸ºéæ³•æ—¥æœŸã€‚  
    ä½ çš„ä»»åŠ¡æ˜¯è¯†åˆ«è¾“å…¥ä¸­æ‰€æœ‰éæ³•æ—¥æœŸçš„è¡Œå·ã€åŸå§‹æ—¥æœŸæ–‡æœ¬å’Œéæ³•åŸå› ï¼Œå¹¶è¿”å›è¿™äº›ä¿¡æ¯ã€‚

    **åˆæ³•æ—¥æœŸå®šä¹‰**ï¼š  
    å¯æ˜ç¡®è§£æä¸ºæœ‰æ•ˆæ—¥æœŸï¼ˆç¬¦åˆå…¬å†è§„åˆ™ï¼‰ï¼Œå³ä½¿åŒ…å«ç®€å•ç¬”è¯¯ï¼Œä¾‹å¦‚ï¼š  
    - "2025å¹´10æœˆ  28æ—¥" â†’ è§£æä¸º "2025å¹´10æœˆ28æ—¥"  
    - "2025å¹´10.28æ—¥" â†’ è§£æä¸º "2025å¹´10æœˆ28æ—¥"  
    - "2025å¹´1028" â†’ è§£æä¸º "2025å¹´10æœˆ28æ—¥"  
    - "20251028" â†’ è§£æä¸º "2025å¹´10æœˆ28æ—¥"  
    - "2025å¹´10æœˆ5å·" â†’ è§£æä¸º "2025å¹´10æœˆ5æ—¥"  
    - "9.13" â†’ è§£æä¸º "2025å¹´9æœˆ13æ—¥"  
    - "2025.10.1è¡¥" è§£æä¸º "2025å¹´10æœˆ1æ—¥"  

    **éæ³•æ—¥æœŸå®šä¹‰**ï¼š  
    1. æ—¥æœŸæœ‰æ­§ä¹‰ï¼Œæ— æ³•æ˜ç¡®è§£æï¼Œä¾‹å¦‚ï¼š  
       - "2025å¹´9223" â†’ æ— æ³•åˆ¤æ–­æ˜¯9æœˆ22æ—¥è¿˜æ˜¯9æœˆ23æ—¥  
    2. æ—¥æœŸæ˜æ˜¾æ— æ•ˆï¼ˆä¾‹å¦‚æœˆä»½æˆ–æ—¥æœŸè¶…å‡ºæœ‰æ•ˆèŒƒå›´ï¼‰ï¼Œä¾‹å¦‚ï¼š  
       - "2025å¹´13æœˆ28æ—¥" â†’ 13æœˆä¸å­˜åœ¨  
       - "2025å¹´2æœˆ30æ—¥" â†’ 2æœˆæ— 30æ—¥  
    3. ä¸æ˜¯å½“å¹´æ•°æ®ï¼Œä¾‹å¦‚ï¼š  
       - "2024å¹´10æœˆ28æ—¥" â†’ å¹´ä»½ä¸å¯¹ï¼Œå½“å‰æ—¶é—´æ˜¯{int(datetime.now().strftime('%Y%m%d'))}å¹´ 
       - "2028å¹´10æœˆ28æ—¥" â†’ å¹´ä»½ä¸å¯¹ï¼Œå½“å‰æ—¶é—´æ˜¯ {int(datetime.now().strftime('%Y%m%d'))} å¹´

    **è¾“å…¥æ ¼å¼**ï¼š  
    JSON æ•°ç»„ï¼Œæ¯é¡¹åŒ…å«è¡Œå·å’Œæ—¥æœŸå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ï¼š  
    ```json
    [
        {{ "è¡Œå·": 44, "æ—¥æœŸï¼ˆå¿…å¡«ï¼‰": "2025å¹´9æœˆ21æ—¥" }},
        {{ "è¡Œå·": 45, "æ—¥æœŸï¼ˆå¿…å¡«ï¼‰": "2025å¹´9117æ—¥" }}
    ]
    ```

    **ä»»åŠ¡ï¼Œ ç°åœ¨å¼€å§‹å›ç­”æˆ‘çš„é—®é¢˜ï¼Œjsonæ ¼å¼è¿”å›**ï¼š  
    æ£€æŸ¥è¾“å…¥ `{date_json}` ä¸­çš„æ¯ä¸ªæ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ‰¾å‡ºæ‰€æœ‰éæ³•æ—¥æœŸã€‚  
    - å¦‚æœå­˜åœ¨éæ³•æ—¥æœŸï¼Œè¿”å›åŒ…å«æ¯é¡¹éæ³•æ—¥æœŸçš„è¡Œå·ã€åŸå§‹æ—¥æœŸæ–‡æœ¬å’ŒåŸå› çš„ JSON æ•°ç»„ã€‚  
    - å¦‚æœæ²¡æœ‰éæ³•æ—¥æœŸï¼Œè¿”å›ç©ºæ•°ç»„ `[]`ã€‚  

    **è¿”å›æ ¼å¼**ï¼š  
    ```json
    [
        {{
            "è¡Œå·": int,
            "æ—¥æœŸï¼ˆå¿…å¡«ï¼‰": "åŸå§‹éæ³•æ—¥æœŸæ–‡æœ¬",
            "reason": "åˆ¤å®šä¸ºéæ³•çš„åŸå› "
        }}
    ]
    ```

    **ç¤ºä¾‹**ï¼š  
    è¾“å…¥ï¼š  
    ```json
    [
        {{ "è¡Œå·": 44, "æ—¥æœŸï¼ˆå¿…å¡«ï¼‰": "2025å¹´9æœˆ21æ—¥" }},
        {{ "è¡Œå·": 45, "æ—¥æœŸï¼ˆå¿…å¡«ï¼‰": "2025å¹´9117æ—¥" }}
    ]
    ```  
    è¾“å‡ºï¼š  
    ```json
    [
        {{
            "è¡Œå·": 45,
            "æ—¥æœŸï¼ˆå¿…å¡«ï¼‰": "2025å¹´9117æ—¥",
            "reason": "æ—¥æœŸæœ‰æ­§ä¹‰ï¼Œæ— æ³•è§£æä¸ºæœ‰æ•ˆæ—¥æœŸ"
        }}
    ]
    ```  
    è¾“å…¥ï¼ˆå…¨åˆæ³•ï¼‰ï¼š  
    ```json
    [
        {{ "è¡Œå·": 44, "æ—¥æœŸï¼ˆå¿…å¡«ï¼‰": "2025å¹´9æœˆ21æ—¥" }}
    ]
    ```  
    è¾“å‡ºï¼š  
    ```json
    []
    ```
    """
    response = model_client.get_completion(prompt)
    try:
        response = parse_json_markdown(response)
        return response
    except:
        raise ValueError(prompt, response)

def check_date_for_df(df: pd.DataFrame):
    df['è¡Œå·'] = df.index + 2
    data = df[['è¡Œå·', 'æ—¥æœŸï¼ˆå¿…å¡«ï¼‰']].to_dict(orient='records')
    data = json.dumps(data, ensure_ascii=False, indent=2)

    res = is_legal_date_batch(data)
    print('-----------------æ—¥æœŸæ£€æµ‹è¿”å›ç»“æœ----------------')
    print(res)
    print('---------------------------------------------')
    if res:
        summary_text = 'ä¸Šä¼ æ–‡ä»¶çš„æ—¥æœŸ[å¿…å¡«]å­—æ®µæœ‰é—®é¢˜ï¼Œè¯·å…ˆè§£å†³æ—¥æœŸé—®é¢˜å†ä¸Šä¼ å¤„ç†ï¼š\n'
        for item in res:
            row_name = item['è¡Œå·']
            date_value = item['æ—¥æœŸï¼ˆå¿…å¡«ï¼‰']
            reason = item['reason']
            summary_text += f"æ•°æ®ç¬¬{row_name}è¡Œæ—¥æœŸæœ‰é—®é¢˜: {date_value} {reason}\n"
        return {
            'is_legal': False,
            'summary_text': summary_text
        }
    else:
        return {
            'is_legal': True,
            'summary_text': ''
        }
    
 

def process_ahead(excel_path: str, selected_halls: List[str]) -> Dict[str, Any]:
    """
    é¢„æ ¡éªŒå‡½æ•°ï¼Œåªæ ¡éªŒæ•°æ®é‡å’Œæ—¥æœŸæ ¼å¼

    Args:
        excel_path: Excelæ–‡ä»¶è·¯å¾„
        selected_halls: é€‰æ‹©çš„å…å·åˆ—è¡¨

    Returns:
        Dict: åŒ…å«æ ¡éªŒç»“æœå’Œé”™è¯¯ä¿¡æ¯çš„å­—å…¸
    """
 
    # 1. è¯»å–Excelæ–‡ä»¶
    df = pd.read_excel(excel_path, sheet_name=0)

    # 2. æ£€æŸ¥ç­›é€‰åçš„æ•°æ®æ˜¯å¦ä¸ºç©º
    if not selected_halls or len(selected_halls) == 0:
        return {
            'valid': False,
            'errors': ["å¿…é¡»è‡³å°‘é€‰æ‹©ä¸€ä¸ªå…å·è¿›è¡Œå¤„ç†"]
        }

    # æ‹¼æ¥å…å·ç­›é€‰æ¡ä»¶
    hall_filter = '|'.join(selected_halls)
    df_filtered_by_halls = df[df['å…å·ï¼ˆå¿…å¡«ï¼‰'].str.contains(hall_filter, na=False)]

    if len(df_filtered_by_halls) == 0:
        return {
            'valid': False,
            'errors': [f"ç­›é€‰åçš„æ•°æ®ä¸ºç©ºï¼Excelæ–‡ä»¶ä¸­æœªæ‰¾åˆ°åŒ…å«ä»¥ä¸‹å…å·çš„æ•°æ®: {', '.join(selected_halls)}"]
        }

    # 3. æ ¡éªŒæ—¥æœŸå­—æ®µ
    print('tolist-----------')
    print(df_filtered_by_halls['æ—¥æœŸï¼ˆå¿…å¡«ï¼‰'].tolist())

    date_res = check_date_for_df(df_filtered_by_halls)

    if not date_res['is_legal']:
        # å°†é”™è¯¯ä¿¡æ¯æ ¼å¼åŒ–ä¸ºå‰ç«¯å‹å¥½çš„æ ¼å¼
        error_lines = [line.strip() for line in date_res['summary_text'].strip().split('\n') if line.strip()]
        return {
            'valid': False,
            'errors': error_lines
        }

    return {
        'valid': True,
        'errors': []
    }


def get_date_str_from_text(text):
    PROMPT_DATE_EXTRACT = f'''
    å½“å‰æ—¶é—´æ˜¯: {datetime.now().strftime('%Yå¹´')}

    ä½ æ˜¯ä¸€ä¸ªæ—¶é—´æ—¥æœŸè§£æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ï¼Œè¿”å›è¿™ä¸ªæ–‡ä»¶åˆé€‚çš„æ—¥æœŸå­—ç¬¦ä¸²ã€‚
    
    è¾“å…¥æ–‡æœ¬ï¼š{text}
    
    **é‡è¦è§„åˆ™ï¼š**
    1. å¦‚æœæ–‡ä»¶åä¸­**æ²¡æœ‰æ˜ç¡®çš„æ—¥æœŸä¿¡æ¯**ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸² ''
    2. å¦‚æœæ—¥æœŸä¿¡æ¯**æœ‰æ­§ä¹‰**ï¼ˆå¦‚09223å¯èƒ½æ˜¯0922æˆ–0923ï¼‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸² ''
    3. å¦‚æœæ—¥æœŸä¿¡æ¯**ä¸å®Œæ•´**ï¼ˆå¦‚åªæœ‰æœˆä»½æ²¡æœ‰æ—¥æœŸï¼‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸² ''
    4. åªæœ‰**æ˜ç¡®ä¸”æ— æ­§ä¹‰**çš„æ—¥æœŸæ‰è¿”å› YYYYMMDD æ ¼å¼
    5. æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼çš„è¯†åˆ«å’Œè½¬æ¢
    
    è¿”å›æ ¼å¼ä¸º YYYYMMDD çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œä¸ç¡®å®šæ—¶è¿”å›ç©ºå­—ç¬¦ä¸² ''ã€‚
    
    ç¤ºä¾‹ï¼š
    è¾“å…¥ï¼šæ˜ å®¢10æœˆ03æ—¥æ‰“å¡æ•°æ®.xlsxï¼Œ ä¸”æˆ‘ä»¬å·²çŸ¥å½“å¹´æ˜¯2025å¹´ï¼Œåˆ™
    è¾“å‡ºï¼š20251003
    
    è¾“å…¥ï¼šä¸»æŒæ‰“å¡9.27.xlsx
    è¾“å‡ºï¼š20250927
    
    è¾“å…¥ï¼šæ‰“å¡æ•°æ®.xlsx
    è¾“å‡ºï¼š
    
    è¾“å…¥ï¼šä¸»æŒæ‰“å¡09223.xlsx
    è¿™ä¸ªæœ‰æ­§ä¹‰
    è¾“å‡ºï¼š
    
    è¾“å…¥ï¼š10æœˆæ‰“å¡.xlsx
    è¾“å‡ºï¼š
    
    è¾“å…¥ï¼šä¸»æŒæ‰“å¡2024-10-03.xlsx
    è¾“å‡ºï¼š20241003
    
  

    **åªè¿”å›æ—¥æœŸå­—ç¬¦ä¸²æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼**
    '''
    try:
        response = model_client.get_completion(
            PROMPT_DATE_EXTRACT.format(text=text)
        )
        
        # æ¸…ç†å“åº”å†…å®¹
        response = response.strip().replace("'", "")
        
        # éªŒè¯è¿”å›æ ¼å¼
        if response == '':
            return ''
        
        # æ£€æŸ¥æ˜¯å¦ä¸º8ä½æ•°å­—æ ¼å¼
        if len(response) == 8 and response.isdigit():
            return response
        
        # å¦‚æœä¸ç¬¦åˆæ ¼å¼ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return ''
        
    except Exception as e:
        print(f"æ—¥æœŸæå–å‡ºé”™: {e}")
        return ''


def gen_names():
    
    names = []
    for p in Path(r'F:\vscode_workspace\smart_table\daily_data').glob('*.xlsx'):
        if 'output' in p.stem:
            df = pd.read_excel(p, sheet_name=0)
            for idx, row in df.iterrows():
                host_list_text = row['ä¸»æŒäººå‘˜åˆ—è¡¨_AIè§£æ']
                paimai_list_text = row['æ’éº¦äººå‘˜åˆ—è¡¨_AIè§£æ']
                if host_list_text and paimai_list_text:
                    # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå¤„ç† NaN ç­‰å¼‚å¸¸å€¼
                    host_list_text = str(host_list_text) if host_list_text is not None else ''
                    paimai_list_text = str(paimai_list_text) if paimai_list_text is not None else ''
                    
                    host_list = [name.strip() for name in host_list_text.split('|') if name.strip()]
                    paimai_list = [name.strip() for name in paimai_list_text.split('|') if name.strip()]
                    for host_name in host_list:
                        names.extend(host_name.split('-'))
                    for paimai_name in paimai_list:
                        names.extend(paimai_name.split('-'))
    names = list(set(names))
    # å†™å…¥tempç›®å½•è€Œä¸æ˜¯å½“å‰ç›®å½•
    import tempfile
    temp_dir = tempfile.gettempdir()
    known_names_path = os.path.join(temp_dir, 'known_names_v2.txt')
    with open(known_names_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(names))
        

 

 
if __name__ == '__main__':
    # gen_names()
    # raise 
 
    paths = list(Path('daily_data').glob('*.xlsx'))
    for p in paths:
        p = Path(p)
 
        output_path = p.with_suffix(f'.{CURRENT_MODEL}.v3.output.xlsx')
        # if 'v3' in p.stem:
        #     continue
        if 'output' in p.stem:
            continue 

        date_str_from_file = get_date_str_from_text(p.stem)
        from global_config import STRICT_DATE_FILTER
        process_one_file(p, output_path=output_path, date_str_from_file=date_str_from_file, strict_date_filter=STRICT_DATE_FILTER)
        time.sleep(1)